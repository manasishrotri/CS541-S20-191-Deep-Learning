# -*- coding: utf-8 -*-
"""Homework7_mvshrotri_aspore

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WjOpwEWSHUtt3RN6QCGjTWrkDf7_oFJE
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from tensorflow.keras import layers
from tensorflow import keras 
import tensorflow as tf
from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.python.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Lambda, Input, Dense

from tensorflow.keras.datasets import mnist

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train = X_train.astype('float32') / 255.
X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))
print(X_train.shape)
X_test = X_test.astype('float32') / 255.
X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))
print(X_test.shape)

batch_size = 128
no_epochs = 100
latent_dim = 128

from keras.layers import BatchNormalization
input_img = Input(shape=(784,))
encoded = Dense(600, activation=tf.nn.tanh)(input_img)
encoded=tf.keras.layers.BatchNormalization()(encoded)
# encoded = Dense(200, activation=tf.nn.tanh)(encoded)
# encoded=tf.keras.layers.BatchNormalization()(encoded)
encoded = Dense(128, activation=tf.nn.leaky_relu)(encoded)
encoded=tf.keras.layers.BatchNormalization()(encoded)
# encoded = Dense(72, activation=tf.nn.leaky_relu)(encoded)
# encoded=tf.keras.layers.BatchNormalization()(encoded)
encoded = Dense(32, activation=tf.nn.leaky_relu)(encoded)
z_mean = Dense(latent_dim, name='z_mean')(encoded)
z_log_var = Dense(latent_dim, name='z_log_var')(encoded)

def sampling(args):
    mu, log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(mu)[0], K.int_shape(mu)[1]))
    return mu + K.exp(0.5 * log_sigma) * epsilon

z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

encoder=Model(input_img, [z_mean, z_log_var, z], name='encoder')

encoder.summary()

latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(256, activation=tf.nn.tanh)(latent_inputs)
x=Dense(512,activation=tf.nn.leaky_relu)(x)
x=tf.keras.layers.BatchNormalization()(x)
x=Dense(600,activation=tf.nn.leaky_relu)(x)
x=tf.keras.layers.BatchNormalization()(x)
x=Dense(700,activation=tf.nn.leaky_relu)(x)
x=tf.keras.layers.BatchNormalization()(x)
outputs = Dense(784, activation='sigmoid')(x)
#decoded = Dense(64, activation='relu')(encoded)
#decoded = Dense(128, activation='relu')(decoded)
#decoded = Dense(784, activation='sigmoid')(decoded)

decoder=Model(latent_inputs,outputs,name='decoder')
decoder.summary()

vae_outputs = decoder(encoder(input_img)[2])
vae         = Model(input_img, vae_outputs, name='vae')
vae.summary()

from tensorflow.keras.losses import mse, binary_crossentropy

reconstruction_loss = binary_crossentropy(input_img,vae_outputs)
reconstruction_loss *= 784
kl_loss = 1 + z_log_var - z_mean*z_mean - K.exp(z_log_var)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -0.5
vae_loss = K.mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)
vae.compile(optimizer='adam')#, loss='binary_crossentropy')

vae.fit(X_train, X_train,
                epochs=150,
                batch_size=128,
                shuffle=True,validation_data=(X_test, X_test))

decoded_imgs = vae.predict(X_test)

ax = plt.subplot(2, 15, 0 + 1 + 15)
deo=decoded_imgs[0]
print(deo.shape)
deo=tf.reshape(deo, shape=[28,28])
print(deo.shape)
plt.imshow(deo)
print(decoded_imgs[0].shape)
plt.gray()
ax.get_xaxis().set_visible(True)
ax.get_yaxis().set_visible(True)
plt.figure(figsize=(20, 4))
plt.show
plt.imshow(X_train[0].reshape(28, 28))

print(z_mean,z_log_var)

n = 16 
digit_size=28 # how many digits we will display
plt.figure(figsize=(20, 4))
for i in range(n):
    # display original
    ax = plt.subplot(3, n, i + 1)
    plt.imshow(X_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # display reconstruction
    ax = plt.subplot(3, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    ax = plt.subplot(3, n, i + 1+2 * n)
    # #a= [ random.random() for x in range(32) ]
    a= [ np.random.normal(0,1) for x in range(latent_dim) ]
    z_sample=(np.array([[a]]))
    z_sample=tf.keras.layers.Reshape((128,),name='re')(z_sample)
    x_decoded = decoder.predict(z_sample)
    x_decoded[0].shape
    digit = x_decoded[0].reshape(digit_size, digit_size)
    plt.imshow(digit)
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

plt.show()

# Display a 2D manifold of the digits
from scipy.stats import norm
n = 20  # figure with 20x20 digits
digit_size = 28
batch_size=32
figure = np.zeros((digit_size * n, digit_size * n))

# Construct grid of latent variable values
grid_x = norm.ppf(np.linspace(0.05, 0.95, n))
grid_y = norm.ppf(np.linspace(0.05, 0.95, n))

# decode for each square in the grid
for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]])
        print(z_sample)
        z_sample = np.tile(z_sample, 16).reshape(batch_size)
        x_decoded = decoder.predict(z_sample, batch_size=batch_size)
        digit = x_decoded[0].reshape(digit_size, digit_size)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure, cmap='gnuplot2')
plt.show()